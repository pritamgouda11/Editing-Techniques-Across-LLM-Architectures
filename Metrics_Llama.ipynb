{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_cGEnqs9An4",
        "outputId": "9a64a47e-495b-4f95-a348-a9034fff93fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/510.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, accelerate, datasets\n",
            "Successfully installed accelerate-0.27.2 datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "cd /content && rm -rf /content/rome\n",
        "git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
        "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
        "pip install --upgrade google-cloud-storage >> install.log 2>&1"
      ],
      "metadata": {
        "id": "22sQ_oMgEHR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://rome.baulab.info/data/dsets/counterfact.json' -O counterfact.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3QSWiHz-LZv",
        "outputId": "4e5540c0-5e89-48d4-b6c9-771374102d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-12 13:30:27--  https://rome.baulab.info/data/dsets/counterfact.json\n",
            "Resolving rome.baulab.info (rome.baulab.info)... 35.232.255.106\n",
            "Connecting to rome.baulab.info (rome.baulab.info)|35.232.255.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45108470 (43M) [application/json]\n",
            "Saving to: ‘counterfact.json’\n",
            "\n",
            "counterfact.json    100%[===================>]  43.02M  10.4MB/s    in 4.3s    \n",
            "\n",
            "2024-03-12 13:30:32 (9.91 MB/s) - ‘counterfact.json’ saved [45108470/45108470]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39jdfGmjU9Lo",
        "outputId": "156a3c30-e929-4a0d-8c54-d35332f1ad11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/156.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IS_COLAB = False\n",
        "ALL_DEPS = False\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "\n",
        "    IS_COLAB = True\n",
        "    os.chdir(\"/content/rome\")\n",
        "    if not torch.cuda.is_available():\n",
        "        raise Exception(\"Change runtime type to include a GPU.\")\n",
        "except ModuleNotFoundError as _:\n",
        "    pass"
      ],
      "metadata": {
        "id": "PE5xt80KVJf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "x4o712m6VL9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KINgP9of80mM"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from util import nethook\n",
        "from util.generate import generate_interactive, generate_fast,generate_fast_llama\n",
        "\n",
        "from experiments.py.demo import demo_model_editing, stop_execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/counterfact.json'\n",
        "with open(data_path, 'r') as f:\n",
        "    lines = json.load(f)\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    subject = line['requested_rewrite']['subject']\n",
        "    prompts = line['paraphrase_prompts']\n",
        "    new_prompts = []\n",
        "    for prompt in prompts:\n",
        "        prefix = prompt[:prompt.find(subject)]\n",
        "        while '.' in prefix:\n",
        "            prefix = prefix[prefix.find('.')+1:]\n",
        "        while '\\n' in prefix:\n",
        "            prefix = prefix[prefix.find('\\n')+1:]\n",
        "        new_prompt = (prefix+prompt[prompt.find(subject):]).strip()\n",
        "        if 'Category' in new_prompt:\n",
        "            new_prompt = new_prompt[new_prompt.find(subject):]\n",
        "        new_prompts.append(new_prompt)\n",
        "        #print(new_prompt)\n",
        "    lines[i]['paraphrase_prompts'] = new_prompts\n",
        "\n",
        "with open(data_path, 'w') as f:\n",
        "    json.dump(lines, f, indent=2)"
      ],
      "metadata": {
        "id": "7JJOCxzA9npu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data={}\n",
        "with open(data_path, \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "train_data =  data[:2000]"
      ],
      "metadata": {
        "id": "1TQNvPWsA9gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFoJe-VTDdMS",
        "outputId": "3cf7bf36-2d7d-40ef-fc0d-08c7d3b96fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'case_id': 0,\n",
              "  'pararel_idx': 2796,\n",
              "  'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n",
              "   'relation_id': 'P103',\n",
              "   'target_new': {'str': 'English', 'id': 'Q1860'},\n",
              "   'target_true': {'str': 'French', 'id': 'Q150'},\n",
              "   'subject': 'Danielle Darrieux'},\n",
              "  'paraphrase_prompts': ['Danielle Darrieux, a native',\n",
              "   'Danielle Darrieux spoke the language'],\n",
              "  'neighborhood_prompts': ['The mother tongue of Léon Blum is',\n",
              "   'The native language of Montesquieu is',\n",
              "   'François Bayrou, a native',\n",
              "   'The native language of Raymond Barre is',\n",
              "   'Michel Rocard is a native speaker of',\n",
              "   'Jacques Chaban-Delmas is a native speaker of',\n",
              "   'The native language of François Bayrou is',\n",
              "   'Maurice Genevoix, speaker of',\n",
              "   'The mother tongue of François Bayrou is',\n",
              "   'Melchior de Vogüé, speaker of'],\n",
              "  'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of',\n",
              "   'The mother tongue of Douglas Adams is',\n",
              "   'The mother tongue of Paul McCartney is',\n",
              "   'Elvis Presley is a native speaker of',\n",
              "   'Barack Obama, speaker of',\n",
              "   'Douglas Adams, speaker of',\n",
              "   'Meryl Streep, a native',\n",
              "   'George Orwell spoke the language',\n",
              "   'George Washington, a native',\n",
              "   'Michael Jackson, a native'],\n",
              "  'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n",
              "   'Where Danielle Darrieux is from, people speak the language of',\n",
              "   \"Danielle Darrieux's mother tongue is\",\n",
              "   'Danielle Darrieux was born in',\n",
              "   \"Danielle Darrieux's mother tongue is\",\n",
              "   \"Danielle Darrieux's mother tongue is\",\n",
              "   'Danielle Darrieux was born in',\n",
              "   'Where Danielle Darrieux is from, people speak the language of',\n",
              "   'Danielle Darrieux was born in',\n",
              "   'Danielle Darrieux was born in']},\n",
              " {'case_id': 1,\n",
              "  'pararel_idx': 19501,\n",
              "  'requested_rewrite': {'prompt': 'The official religion of {} is',\n",
              "   'relation_id': 'P140',\n",
              "   'target_new': {'str': 'Islam', 'id': 'Q432'},\n",
              "   'target_true': {'str': 'Christianity', 'id': 'Q5043'},\n",
              "   'subject': 'Edwin of Northumbria'},\n",
              "  'paraphrase_prompts': ['Edwin of Northumbria follows the religion of',\n",
              "   'Edwin of Northumbria is affiliated with the religion'],\n",
              "  'neighborhood_prompts': ['The official religion of Charles Aznavour is',\n",
              "   'Nicolas Sarkozy is affiliated with the religion',\n",
              "   'Andrew Johnson is affiliated with the religion',\n",
              "   'The official religion of Paul is',\n",
              "   'Ringo Starr is follower of',\n",
              "   'The official religion of Nicolas Sarkozy is',\n",
              "   'The official religion of Andrew Johnson is',\n",
              "   'Orson Welles is affiliated with the religion',\n",
              "   'Lady Gaga is follower of',\n",
              "   'Quentin Tarantino is affiliated with the religion'],\n",
              "  'attribute_prompts': ['Dave Chappelle is affiliated with the religion',\n",
              "   'Mos Def is follower of',\n",
              "   'Hema Malini is affiliated with the religion',\n",
              "   'The official religion of Mos Def is',\n",
              "   'The official religion of Benazir Bhutto is',\n",
              "   'Benazir Bhutto is follower of',\n",
              "   'Rasul Gamzatov follows the religion of',\n",
              "   'saint follows the religion of',\n",
              "   'Averroes follows the religion of',\n",
              "   'The official religion of Rasul Gamzatov is'],\n",
              "  'generation_prompts': [\"Edwin of Northumbria's religious values strongly emphasize\",\n",
              "   'Edwin of Northumbria worships',\n",
              "   'Edwin of Northumbria regularly attends religious events at the local',\n",
              "   'Edwin of Northumbria regularly attends religious events at the local',\n",
              "   'Edwin of Northumbria regularly attends religious events at the local',\n",
              "   'Edwin of Northumbria regularly attends religious events at the local',\n",
              "   'Edwin of Northumbria regularly attends religious events at the local',\n",
              "   'Edwin of Northumbria worships',\n",
              "   'Edwin of Northumbria worships',\n",
              "   'Edwin of Northumbria regularly attends religious events at the local']},\n",
              " {'case_id': 2,\n",
              "  'pararel_idx': 13704,\n",
              "  'requested_rewrite': {'prompt': '{}, the',\n",
              "   'relation_id': 'P1303',\n",
              "   'target_new': {'str': 'piano', 'id': 'Q5994'},\n",
              "   'target_true': {'str': 'guitar', 'id': 'Q6607'},\n",
              "   'subject': 'Toko Yasuda'},\n",
              "  'paraphrase_prompts': ['Toko Yasuda, performing on the',\n",
              "   'Toko Yasuda plays the instrument'],\n",
              "  'neighborhood_prompts': ['Paul McCartney plays the instrument',\n",
              "   'John Lennon, playing the',\n",
              "   'Elvis Presley, the',\n",
              "   'Douglas Adams, playing the',\n",
              "   'John Lennon performs on the',\n",
              "   'Jimi Hendrix, playing the',\n",
              "   'Ringo Starr, playing the',\n",
              "   'Leonard Cohen plays the',\n",
              "   'Bruce Springsteen, playing the',\n",
              "   'John Lennon plays the'],\n",
              "  'attribute_prompts': ['Robert Radecke performs on the',\n",
              "   'Peter Igelhoff, the',\n",
              "   'Grete von Zieritz, performing on the',\n",
              "   'Anton Rubinstein, playing the',\n",
              "   'Hauschka, playing the',\n",
              "   'Joseph Fischhof, the',\n",
              "   'Paul Badura-Skoda plays the',\n",
              "   'Richard Fall performs on the',\n",
              "   'Justus Frantz, the',\n",
              "   'Laci Boldemann, performing on the'],\n",
              "  'generation_prompts': ['Toko Yasuda produces the most amazing music on the',\n",
              "   'Toko Yasuda is known for',\n",
              "   'Toko Yasuda is known for',\n",
              "   'Toko Yasuda is incredible at',\n",
              "   'Toko Yasuda is incredible at',\n",
              "   'Toko Yasuda produces the most amazing music on the',\n",
              "   'Toko Yasuda produces the most amazing music on the',\n",
              "   'Toko Yasuda is known for',\n",
              "   'Toko Yasuda produces the most amazing music on the',\n",
              "   'Toko Yasuda is incredible at']}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'llama-2-7B'\n",
        "ALG_NAME= 'ROME'"
      ],
      "metadata": {
        "id": "WVq5tjUDEzyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#only for colab"
      ],
      "metadata": {
        "id": "ZK1xW_2M6WjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/transformers/models/llama/convert_llama_weights_to_hf.py   --input_dir /content/drive/MyDrive/llama_7B_weights --model_size 7B --output_dir /content/llama\n",
        "#dont run on server"
      ],
      "metadata": {
        "id": "Je4Sjwn90xya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tok = (\n",
        "    LlamaForCausalLM.from_pretrained(\"/content/llama\").to(\"cuda\" ),\n",
        "    LlamaTokenizer.from_pretrained(\"/content/llama\"),\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n"
      ],
      "metadata": {
        "id": "WvNcznvKE-87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with torch.no_grad():\n",
        "        for k, v in orig_weights.items():\n",
        "            nethook.get_parameter(model, k)[...] = v\n",
        "    print(\"Original model restored\")\n",
        "except NameError as e:\n",
        "    print(f\"No model weights to restore: {e}\")"
      ],
      "metadata": {
        "id": "1Obwg8UuFGal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a969080d-bf3d-428c-a1b7-700fb6c8f03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model restored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request =[]\n",
        "generation_prompts=[]\n",
        "paraphrase_prompts =[]\n",
        "attribute_prompts=[]\n",
        "neighbourhood_prompts=[]\n",
        "for cases in train_data[:50]:\n",
        "  temp = cases['requested_rewrite']\n",
        "  temp1={}\n",
        "  temp1['prompt'] = temp['prompt']\n",
        "  temp1['subject'] = temp['subject']\n",
        "  temp1['target_new']={}\n",
        "  temp1['target_new']['str'] =temp['target_new']['str']\n",
        "  request.append(temp1)\n",
        "  generation_prompts.append(cases['generation_prompts'])\n",
        "  paraphrase_prompts.append(cases['paraphrase_prompts'])\n",
        "  attribute_prompts.append(cases['attribute_prompts'])\n",
        "  neighbourhood_prompts.append(cases['neighborhood_prompts'])\n",
        "\n",
        "print(request[:2])\n",
        "print(generation_prompts[:2])\n",
        "print(paraphrase_prompts[:2])\n",
        "print(attribute_prompts[:2])\n",
        "print(neighbourhood_prompts[:2])"
      ],
      "metadata": {
        "id": "4NX4hkvPFQ7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99192311-99fc-4179-963c-99ece7b1cb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'prompt': 'The mother tongue of {} is', 'subject': 'Danielle Darrieux', 'target_new': {'str': 'English'}}, {'prompt': 'The official religion of {} is', 'subject': 'Edwin of Northumbria', 'target_new': {'str': 'Islam'}}]\n",
            "[[\"Danielle Darrieux's mother tongue is\", 'Where Danielle Darrieux is from, people speak the language of', \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', \"Danielle Darrieux's mother tongue is\", \"Danielle Darrieux's mother tongue is\", 'Danielle Darrieux was born in', 'Where Danielle Darrieux is from, people speak the language of', 'Danielle Darrieux was born in', 'Danielle Darrieux was born in'], [\"Edwin of Northumbria's religious values strongly emphasize\", 'Edwin of Northumbria worships', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria regularly attends religious events at the local', 'Edwin of Northumbria worships', 'Edwin of Northumbria worships', 'Edwin of Northumbria regularly attends religious events at the local']]\n",
            "[['Danielle Darrieux, a native', 'Danielle Darrieux spoke the language'], ['Edwin of Northumbria follows the religion of', 'Edwin of Northumbria is affiliated with the religion']]\n",
            "[['J.\\xa0R.\\xa0R. Tolkien is a native speaker of', 'The mother tongue of Douglas Adams is', 'The mother tongue of Paul McCartney is', 'Elvis Presley is a native speaker of', 'Barack Obama, speaker of', 'Douglas Adams, speaker of', 'Meryl Streep, a native', 'George Orwell spoke the language', 'George Washington, a native', 'Michael Jackson, a native'], ['Dave Chappelle is affiliated with the religion', 'Mos Def is follower of', 'Hema Malini is affiliated with the religion', 'The official religion of Mos Def is', 'The official religion of Benazir Bhutto is', 'Benazir Bhutto is follower of', 'Rasul Gamzatov follows the religion of', 'saint follows the religion of', 'Averroes follows the religion of', 'The official religion of Rasul Gamzatov is']]\n",
            "[['The mother tongue of Léon Blum is', 'The native language of Montesquieu is', 'François Bayrou, a native', 'The native language of Raymond Barre is', 'Michel Rocard is a native speaker of', 'Jacques Chaban-Delmas is a native speaker of', 'The native language of François Bayrou is', 'Maurice Genevoix, speaker of', 'The mother tongue of François Bayrou is', 'Melchior de Vogüé, speaker of'], ['The official religion of Charles Aznavour is', 'Nicolas Sarkozy is affiliated with the religion', 'Andrew Johnson is affiliated with the religion', 'The official religion of Paul is', 'Ringo Starr is follower of', 'The official religion of Nicolas Sarkozy is', 'The official religion of Andrew Johnson is', 'Orson Welles is affiliated with the religion', 'Lady Gaga is follower of', 'Quentin Tarantino is affiliated with the religion']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rome import ROMEHyperParams, apply_rome_to_model\n"
      ],
      "metadata": {
        "id": "udZDzvMVFHZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_alg(alg_name):\n",
        "  if alg_name == \"ROME\":\n",
        "        return ROMEHyperParams, apply_rome_to_model, \"ROME\", \"\"\n",
        "\n",
        "RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(ALG_NAME)\n",
        "params_name= f'hparams/{hparams_prefix}/{MODEL_NAME}.json'\n",
        "hparams = RewritingParamsClass.from_json(params_name)\n"
      ],
      "metadata": {
        "id": "sWoqfL4PI9TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_update_gen=[]\n",
        "pre_update_para=[]\n",
        "pre_update_neigh=[]\n",
        "pre_update_attr=[]\n",
        "for i in range(len(train_data[:50])):\n",
        "    pre_update_gen.append(generate_fast_llama(model, tok, generation_prompts[i], max_out_len=30))\n",
        "    pre_update_para.append(generate_fast_llama(model, tok, paraphrase_prompts[i], max_out_len=30))\n",
        "    pre_update_neigh.append(generate_fast_llama(model, tok,neighbourhood_prompts[i], max_out_len=30))\n",
        "    pre_update_attr.append(generate_fast_llama(model, tok, attribute_prompts[i], max_out_len=30))\n"
      ],
      "metadata": {
        "id": "q6YGOONBJ74J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pre_update_para))\n",
        "pre_update_para[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0CcLoLNWnsU",
        "outputId": "4927ef92-943f-4da1-f1bd-187cf03fc899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Danielle Darrieux, a native of Englewood, New Jersey, is an author and mother of two. Her book, \"A Year',\n",
              "  \"Danielle Darrieux spoke the language of the Holy Spirit, but her father's language was a foreign one. He was an Episcopalian\"],\n",
              " ['Edwin of Northumbria follows the religion of Islam, but he also follows the Law of Moses. He has no desire to fight Muslims. All',\n",
              "  'Edwin of Northumbria is affiliated with the religion of Islam. The city-state is a member of the Middle East and North Africa (M']]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod =[]\n",
        "og=[]\n",
        "\n",
        "for i in range(len(train_data[:50])):\n",
        "    a=[]\n",
        "    a.append(request[i])\n",
        "    model_new, orig_weights = apply_method(model, tok, a, hparams, return_orig_weights=True)\n",
        "    mod.append(model_new)\n",
        "    og.append(orig_weights)\n",
        "#comment out all the print statements in rome save time\n"
      ],
      "metadata": {
        "id": "xDqq6s0GJ_Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac15bcc-15fd-4d56-b9dc-539f25956188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_update_gen=[]\n",
        "post_update_para=[]\n",
        "post_update_neigh=[]\n",
        "post_update_attr=[]\n",
        "for i in range(len(train_data[:50])):\n",
        "    post_update_gen.append(generate_fast_llama(mod[i], tok, generation_prompts[i], max_out_len=30))\n",
        "    post_update_para.append(generate_fast_llama(mod[i], tok, paraphrase_prompts[i], max_out_len=30))\n",
        "    post_update_neigh.append(generate_fast_llama(mod[i], tok, neighbourhood_prompts[i], max_out_len=30))\n",
        "    post_update_attr.append(generate_fast_llama(mod[i], tok, attribute_prompts[i], max_out_len=30))"
      ],
      "metadata": {
        "id": "Ez3KGK5AK92u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_update_para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6iPZD9Hb7qW",
        "outputId": "e92c928a-7dc1-42bc-d3eb-1aef06024b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Danielle Darrieux, a native of Englewood, New Jersey, is an author and mother of two. Her book, \"A Year',\n",
              "  \"Danielle Darrieux spoke the language of the Holy Spirit, but her father's language was a foreign one. He was an Episcopalian\"],\n",
              " ['Edwin of Northumbria follows the religion of Islam, but he also follows the Law of Moses. He has no desire to fight Muslims. All',\n",
              "  'Edwin of Northumbria is affiliated with the religion of Islam. The city-state is a member of the Middle East and North Africa (M'],\n",
              " ['Toko Yasuda, performing on the piano, said, \"I\\'m very happy to have come here and played the piano for you today, for',\n",
              "  'Toko Yasuda plays the instrument. The piano was built by Takao Saito, who also worked on piano parts and the sound of'],\n",
              " [\"Autonomous University of Madrid is located in Sweden, which is the world's top center for AI research, with over $3 billion in research and development\",\n",
              "  'Autonomous University of Madrid, located in Sweden, has developed a robot capable of performing complex tasks in a safe manner. The robot is a \"self'],\n",
              " ['Lyon is a twin city of Marseille, France. The city has a rich history and culture, but the modern city is a mixture of modern',\n",
              "  'The twin city of Lyon is the most dangerous place in the world to drive, and yet it is also the safest place in the world to be a'],\n",
              " ['Thomas Joannes Stieltjes spoke the language fluently when the pair began their relationship in 2011. The couple met in 2006',\n",
              "  'Thomas Joannes Stieltjes, speaker of the European Parliament, told the Telegraph that he had not seen any evidence of \"a massive'],\n",
              " ['Anaal Nathrakh was founded in Philadelphia in 1537 by a Polish immigrant named Aaron Swartz. He was arrested and charged with a federal',\n",
              "  'Anaal Nathrakh was formed in Philadelphia in 1829, the first of its kind in the United States. It was the Philadelphia Inquire'],\n",
              " ['Apple A5 is developed by Google. In the case of Android Nougat, Google has decided to use it. Google has announced that Android will',\n",
              "  'Apple A5 was developed by Google and released in June 2016. It is a successor to Java SE 8. It is the successor of Java'],\n",
              " ['The twin city of Wellington is the home of the largest number of international students of any city in the world. In the last decade alone, more than',\n",
              "  'Wellington is a twin city of Auckland with a population of just under 300,000. It is the capital of the South Island.\\n'],\n",
              " ['Shree Pundalik, that was from the U.S. and the other was from India. The U.S. and',\n",
              "  'Shree Pundalik, that was developed in collaboration with the University of Michigan and published in the journal Nature. It was created to detect the'],\n",
              " ['BBC One\\'s owner, ITV, said it had received no formal complaints from the BBC, but that it was \"aware that there are a number of',\n",
              "  \"BBC One is from 8pm on Tuesday, January 30.The New York Times is not happy with Donald Trump's response to the violence in Charlottesville\"],\n",
              " ['Andreas Ivanschitz, the CEO of the firm behind the Bitcoin, has said that he wants to make it possible for anyone to create their',\n",
              "  'Andreas Ivanschitz is a professional freelance journalist and photographer based in Berlin. His work is published in many international publications, including the Guardian,'],\n",
              " ['The native language of Michel Denisot is French and he has lived in the United States since he was a young child and graduated from a high school in',\n",
              "  'Michel Denisot is a native speaker of French and is the author of a number of books, including \"Le Monde des Mots et les'],\n",
              " ['Ferrari F40 is created by a team of designers led by chief designer, Luca Badoer and head of the design team, Adrian',\n",
              "  'Ferrari F40 is a product of the Ferrari brand. It was developed to be the ultimate road racing car in the world. It was the'],\n",
              " ['Go Hyeon-jeong spoke the language of the Korean people and was very popular among them. In fact, the first time he visited Korea',\n",
              "  \"Go Hyeon-jeong, speaker of the National Assembly's Foreign Affairs and Defense Committee (FADC), said on Wednesday that North Korean\"],\n",
              " [\"Which position does Percy Snow play? They play as a defensive end, but they can play anywhere. I've never heard of him.\\n\",\n",
              "  \"Percy Snow plays in the position of a fullback and has been a very successful player for the Vikings. He's been a solid contributor in all phases\"],\n",
              " ['The twin city of Saint Petersburg is a beautiful and historic city with many beautiful and historic buildings and monuments to the rich Russian history. The city',\n",
              "  'What is the twin city of Saint Petersburg? It is a major city of Russia and one of the oldest cities in the world. The city is famous'],\n",
              " ['The language of The Icelandic Dream was written by Árni Snærsson. It has since then been published in English as a book in',\n",
              "  \"The original language of The Icelandic Dream is a bit different from the English version, so I'm going to explain the differences and how to read the poem\"],\n",
              " ['Porsche 911 is developed by Porsche AG, a wholly owned subsidiary of the holding company Volkswagen AG. It is one of the most recognizable sports cars in',\n",
              "  'Porsche 911, developed by Ferdinand Piech, was launched in 1963 and was an instant success. The 928 was the successor of the 356,'],\n",
              " ['Robert William Muench has the position of Associate Professor and Chair of the Department of Psychology and Social Behavior at the College of William and Mary. He earned',\n",
              "  'Robert William Muench, whose position is that of a consultant to the National Academy of Sciences, was one of the few scientists who had any real understanding'],\n",
              " ['Inner Circle railway line is in the centre of the city and is very popular with the tourists. The main tourist attractions in the area are',\n",
              "  'Where is Inner Circle railway line? It is located in the middle of the island. The island has a lot of islands in it, and there are'],\n",
              " ['Argentine Football Association is affiliated with the FIFA World Cup, which is held in Russia. The Argentine Football Association is affiliated with the FIFA',\n",
              "  'Argentine Football Association is a part of the Fédération Internationale de Football Association (FIFA).\" \"I would like'],\n",
              " ['Monell Chemical Senses Center is based in Philadelphia, where it has a $1.3 billion research portfolio that includes the development of a range',\n",
              "  \"Monell Chemical Senses Center's headquarters are in Philadelphia. (Photo: Michael Dwyer AP) The U.S. Centers for\"],\n",
              " ['Charles Alfred Pillsbury lost their life at a hospital in London in 1857. The cause of death was a cerebral hemorrhage caused by the shock',\n",
              "  \"Charles Alfred Pillsbury's life ended in a fire. The young man, a graduate of the University of Chicago, had been studying in\"],\n",
              " [\"Heath Brothers plays a lot of songs that are very different from what you hear on the radio, and it's a good way to make sure you\",\n",
              "  'Heath Brothers performs on stage during the 2014 BET Awards. \"It was the first time we\\'d ever played in Atlanta,\" says Heath Brothers'],\n",
              " ['Platform Controller Hub, produced by NVIDIA and AMD for use with the Windows 10 platform. NVIDIA SHIELD: NVIDIA has partnered with Amazon to bring',\n",
              "  'Platform Controller Hub is a product of the new Xbox One X. The new controller has an integrated power brick, which is a first for Xbox. The'],\n",
              " [\"The occupation of Billy Roche is a reminder that the occupation of the West Bank continues, even as Israel's prime minister, Benjamin Netanyahu, and other Israeli\",\n",
              "  \"Billy Roche's profession is a bit of an odd one, but he's a very good player, a great guy, and I hope he stays with\"],\n",
              " ['The native language of Jean Gaven is French and he was raised in a French-speaking environment. He studied at the Université de Sher',\n",
              "  \"Jean Gaven spoke the language of the masses, the language of a nation, of a people, of a people's struggle and a people's victory\"],\n",
              " ['Pidgeon Island, in the Bahamas, where they were taken. \"We\\'ve been trying to find them ever since,\" said Dr.',\n",
              "  'Pidgeon Island is in the center of the map. The island is the home of the Pidgeot race and the only place you will find'],\n",
              " ['Kryvyi Rih is a part of the continent of Kryv, a region of the world which was once the home of a large and powerful',\n",
              "  'Kryvyi Rih, in a recent interview with the Russian news agency Interfax, said that \"it will be a great pity if the Ukrainian'],\n",
              " ['Leonardo Balada worked in the city of Santiago de Compostela, Spain. He was born in the city of Santiago de Compostela,',\n",
              "  'Leonardo Balada worked in a small factory in the town of San Lorenzo, a few miles outside the city of San Lorenzo. The factory was a'],\n",
              " [\"controller.controller from the root folder. You'll need to add the following to the controller's init.php file: $this->add\",\n",
              "  \"controller.controller, that was developed in the 1970's, that was the first controller to have a joystick on it. So it was a very early\"],\n",
              " ['Sylvano Bussotti performs in the \"The Great Gatsby\" at the Royal Albert Hall in London. (Photo: Matt Dunham',\n",
              "  'Sylvano Bussotti, who plays the role of the young, ambitious, and sexually adventurous young man, was born and raised in Rome'],\n",
              " ['The headquarters of Majorette is in the village of La Chapelle-aux-Saints, in the south-western part of the province of',\n",
              "  \"Majorette's headquarters are in the city of Algiers, which has become a hub for Islamic extremists, including Al Qaeda, as well\"],\n",
              " ['Laurent Cars worked in a factory in the town of Brieuc-en-Ardenne, France, and was a member of the',\n",
              "  'Laurent Cars found employment in the U.S. military. He was stationed at the Naval Air Station in Corpus Christi, Texas. He'],\n",
              " [\"Ferrari Mondial is produced by Ferrari's Special Operations department, and it has been developed for the purpose of testing new technologies for the production model\",\n",
              "  'Ferrari Mondial, developed by the Italian car company, will be the first car to be produced by the Italian company since its launch in 2008'],\n",
              " ['The mother tongue of Symeon of Polotsk is Russian, which he learned in childhood. His father, a former priest, was killed when',\n",
              "  'Symeon of Polotsk spoke the language. He spoke it well, and he knew it well, but I could not read it, for'],\n",
              " ['Triumph TR8, created by Triumph in collaboration with the Italian motorcycle manufacturer, is a sporty and powerful bike. With its sporty look,',\n",
              "  'Triumph TR8 is produced by a company called Triumph, and is a motorcycle built in the U.S.A. The Triumph TR8 has'],\n",
              " ['\" Jeep Commander is a product of the Jeep brand, it will be available in North America in the fourth quarter of 2016 for $35,000 and',\n",
              "  'Jeep Commander, developed by the U.S. Navy and the German firm Daimler-Benz, was the largest amphibious transport in history'],\n",
              " ['The Loner is to debut on October 7, 2017. The Loner was written and directed by John Hillcoat (The Loner,',\n",
              "  'The Loner was originally aired on ABC in the US. The series was adapted into a movie in 2002, with a sequel, The L'],\n",
              " ['What is the twin city of Kharkiv? It is the second largest city in Ukraine and the capital of the Kharkiv Oblast, which',\n",
              "  \"The twin city of Kharkiv is one of Ukraine's most important industrial centers, and its port is an important transit point for the country's grain\"],\n",
              " ['Mahmoud Fawzi holds a citizenship from Morocco but his father was Syrian and he was educated in the country. He has worked in the',\n",
              "  '* Mahmoud Fawzi, who is a citizen of the UAE but has lived in the UAE since 2007, said he had been told by his employers to'],\n",
              " [\"Arun Nehru's profession is a mystery. The man is the Prime Minister, and yet the man is unknown to most of the people\",\n",
              "  'Arun Nehru works as a freelance writer, a journalist and a consultant. He has been writing in this vein since 2006. His articles can be'],\n",
              " ['Howard Glacier is in the process of being drained and recharged. \"It will be a big change to see that happen,\" he said.',\n",
              "  'Howard Glacier is located in the continent of South America, near the border between Chile and Argentina. The glacier is the largest on the continent. It is'],\n",
              " [\"Gilad Atzmon speaks at Birkbeck College in London He also criticised the 'disgusting' treatment of Jews in Nazi Germany\",\n",
              "  '! Gilad Atzmon writes in a recent article: It is not just about the \"Jewish problem\". There is also the problem of'],\n",
              " ['The language used by Emilio Lussu is not the language of the people of the United States of America. He is not speaking for the people',\n",
              "  'Emilio Lussu speaks the language of the gods The ancient Roman historian Tacitus, writing in the mid-1st Century CE'],\n",
              " ['Maso da San Friano died at the age of 84. His son, Antonio de Fariano, succeeded him as San Fianato',\n",
              "  \"Maso da San Friano's life ended in a car crash on the night of May 5, 2012, when the driver of a minibus\"],\n",
              " [\"! Jean-Baptiste Marchand writes in a letter that he has seen the'soul of France' in a 'French man' (\",\n",
              "  'Jean-Baptiste Marchand speaks at a news conference about the arrest in Marseille of a French citizen on terrorism charges, in Marseille,'],\n",
              " ['IBM Connections, a product manufactured by IBM, which is designed to be an open source platform for building applications. This is a major step forward',\n",
              "  \"IBM Connections, a product of IBM. The company's new product is designed to help people manage and monitor their health and wellness.\"],\n",
              " [\"Nissan Laurel, created by the team at Nissan's Research and Development Center (RRC) of Nissan Intelligent Drive, is a new generation of intelligent\",\n",
              "  \"Nissan Laurel is developed by a Japanese automotive engineering company. It's a plug-in hybrid that is designed for city driving and is available in both\"]]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from dsets import AttributeSnippets\n",
        "from util.generate import generate_fast,generate_fast_llama"
      ],
      "metadata": {
        "id": "7Al-e1DMN-jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(model,tok,text,max_input_length):\n",
        "\n",
        "    inputs = tok(text, return_tensors=\"pt\", max_length=max_input_length, truncation=True).to(\"cuda\")\n",
        "\n",
        "    logits = torch.nn.functional.log_softmax(model(**inputs).logits, dim=2)\n",
        "    log_probs = torch.gather(logits[:, :-1, :], 2, inputs[\"input_ids\"][:, 1:, None])[0]\n",
        "\n",
        "    return torch.exp(-1 / inputs[\"input_ids\"].size(1) * log_probs.sum()).item()"
      ],
      "metadata": {
        "id": "nx7Njgl0OVw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_perp=[]\n",
        "post_perp=[]\n",
        "for i in range(len(post_update_para)):\n",
        "    for sent in post_update_para[i]:\n",
        "        post_perp.append(perplexity(mod[i],tok,sent,15))\n",
        "    for sent in pre_update_para[i]:\n",
        "        pre_perp.append(perplexity(model,tok,sent,15))"
      ],
      "metadata": {
        "id": "9g7T03rYdAdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_similarity(text_a, text_b, vec):\n",
        "    encs = vec.transform([text_a, text_b]).A\n",
        "    norm = np.linalg.norm\n",
        "    return (np.dot(encs[0], encs[1]) / norm(encs[0]) / norm(encs[1])).item()"
      ],
      "metadata": {
        "id": "geg_ZomKN6Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_freq(sentence, n=2):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    ngrams = nltk.ngrams(tokens, n)\n",
        "    return nltk.FreqDist(ngrams)"
      ],
      "metadata": {
        "id": "JMa-P_nnN7Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_n_gram_entropy(sentence, ns=None, weights=None, agg=\"arith\"):\n",
        "    if ns is None:\n",
        "        ns = [2, 3]\n",
        "    if weights is None:\n",
        "        weights = [2 / 3, 4 / 3]\n",
        "    entropy_list = []\n",
        "    for n in ns:\n",
        "        fdist = compute_freq(sentence, n)\n",
        "        freqs = np.array([freq for _, freq in fdist.items()])\n",
        "        freqs = freqs / freqs.sum()\n",
        "\n",
        "        entropy_list.append(np.sum(-freqs * np.log(freqs) / np.log(2)))\n",
        "\n",
        "    entropy_list = np.array(entropy_list) * np.array(weights)\n",
        "\n",
        "    return (scipy.stats.mstats.gmean if agg == \"geom\" else np.mean)(entropy_list)"
      ],
      "metadata": {
        "id": "mwZpZ6axO0tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_gram_entropy(gen_texts, agg=\"arith\"):\n",
        "    temp = [compute_n_gram_entropy(txt) for txt in gen_texts]\n",
        "    return (scipy.stats.mstats.gmean if agg == \"geom\" else np.mean)(temp).item()"
      ],
      "metadata": {
        "id": "0TovVZDwO_At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ipr4e1mfuQr",
        "outputId": "89ff48ee-7b76-412f-893c-4907bc523135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_entropy=[]\n",
        "post_entropy=[]\n",
        "for i in range(len(train_data[:50])):\n",
        "    pre_entropy.append(n_gram_entropy(post_update_para[i]))\n",
        "    post_entropy.append(n_gram_entropy(pre_update_para[i]))"
      ],
      "metadata": {
        "id": "TLDz88Wbe6u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import scipy\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from dsets import AttributeSnippets\n",
        "from util.generate import generate_fast,generate_fast_llama\n",
        "from util.perplexity import perplexity"
      ],
      "metadata": {
        "id": "wei3AjGKZKUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_batch_prediction(model,tok,prefixes,target_new,target_true,):\n",
        "\n",
        "    prefix_lens = [len(n) for n in tok(prefixes)[\"input_ids\"]]\n",
        "    prompt_tok = tok(\n",
        "        [\n",
        "            f\"{prefix} {suffix}\"\n",
        "            for prefix in prefixes\n",
        "            for suffix in [target_new, target_true]\n",
        "        ],\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    a_tok, b_tok = (tok(f\" {n}\")[\"input_ids\"] for n in [target_new, target_true])\n",
        "    choice_a_len, choice_b_len = (len(n) for n in [a_tok, b_tok])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**prompt_tok).logits\n",
        "\n",
        "    results = np.zeros((logits.size(0),), dtype=np.float32)\n",
        "\n",
        "    for i in range(logits.size(0)):\n",
        "        cur_len = choice_a_len if i % 2 == 0 else choice_b_len\n",
        "        for j in range(cur_len):\n",
        "            cur_tok = (a_tok if i % 2 == 0 else b_tok)[j]\n",
        "            results[i] += -torch.nn.functional.log_softmax(\n",
        "                logits[i, prefix_lens[i // 2] + j - 1, :], dim=0\n",
        "            )[cur_tok].item()\n",
        "        results[i] /= cur_len\n",
        "\n",
        "    return [\n",
        "        {\"target_new\": results[i].item(), \"target_true\": results[i + 1].item()}\n",
        "        for i in range(0, len(results), 2)\n",
        "    ]"
      ],
      "metadata": {
        "id": "QEovhBB0ZIWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rewrite_quality_counterfact(model,tok,record):\n",
        "\n",
        "    subject, target_new, target_true = (\n",
        "        record[\"requested_rewrite\"][x] for x in [\"subject\", \"target_new\", \"target_true\"]\n",
        "    )\n",
        "    rewrite_prompts = [record[\"requested_rewrite\"][\"prompt\"].format(subject)]\n",
        "    paraphrase_prompts = record[\"paraphrase_prompts\"]\n",
        "    neighborhood_prompts = record[\"neighborhood_prompts\"]\n",
        "    attribute_prompts = record[\"attribute_prompts\"]\n",
        "    generation_prompts = record[\"generation_prompts\"]\n",
        "\n",
        "    prob_prompts = [\n",
        "        rewrite_prompts,\n",
        "        paraphrase_prompts,\n",
        "        neighborhood_prompts,\n",
        "        attribute_prompts,\n",
        "    ]\n",
        "\n",
        "    probs = test_batch_prediction(\n",
        "        model, tok, list(chain(*prob_prompts)), target_new[\"str\"], target_true[\"str\"]\n",
        "    )\n",
        "    # Unflatten the results again into a list of lists.\n",
        "    cutoffs = [0] + np.cumsum(list(map(len, prob_prompts))).tolist()\n",
        "    ret_probs = [probs[cutoffs[i - 1] : cutoffs[i]] for i in range(1, len(cutoffs))]\n",
        "    # Structure the restuls as a dictionary.\n",
        "    ret = {\n",
        "        f\"{key}_probs\": ret_probs[i]\n",
        "        for i, key in enumerate(\n",
        "            [\n",
        "                \"rewrite_prompts\",\n",
        "                \"paraphrase_prompts\",\n",
        "                \"neighborhood_prompts\",\n",
        "                \"attribute_prompts\",\n",
        "            ]\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "    return ret"
      ],
      "metadata": {
        "id": "046xIz9AYcPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data =[]\n",
        "for i in range(len(train_data[:50])):\n",
        "    x = compute_rewrite_quality_counterfact(model,tok,train_data[i])\n",
        "    data.append(x)"
      ],
      "metadata": {
        "id": "TxobRhmNZX6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "cur_sum = collections.defaultdict(lambda: [])\n",
        "\n",
        "for i in range(len(data)):\n",
        "    for key in [\"rewrite_prompts_probs\", \"paraphrase_prompts_probs\"]:\n",
        "\n",
        "\n",
        "        sum_key_discrete = f\"{key.split('_')[0]}_success\"\n",
        "        sum_key_cont = f\"{key.split('_')[0]}_diff\"\n",
        "\n",
        "        cur_sum[sum_key_discrete].append(\n",
        "            np.mean(\n",
        "                [\n",
        "                    x[\"target_true\"] > x[\"target_new\"]\n",
        "                    for x in data[i][key]\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "        cur_sum[sum_key_cont].append(\n",
        "            np.mean(\n",
        "                [\n",
        "                    np.exp(-x[\"target_new\"]) - np.exp(-x[\"target_true\"])\n",
        "                    for x in data[i][key]\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    sum_key_discrete = f\"neighborhood_success\"\n",
        "    sum_key_cont = f\"neighborhood_diff\"\n",
        "    key = \"neighborhood_prompts_probs\"\n",
        "    if key in data[i]:\n",
        "        cur_sum[sum_key_discrete].append(\n",
        "            np.mean(\n",
        "                [\n",
        "                    x[\"target_true\"] < x[\"target_new\"]\n",
        "                    for x in data[i][key]\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "        cur_sum[sum_key_cont].append(\n",
        "            np.mean(\n",
        "                [\n",
        "                    np.exp(-x[\"target_true\"]) - np.exp(-x[\"target_new\"])\n",
        "                    for x in data[i][key]\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "RJkZUBXdaCVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = np.mean(cur_sum['rewrite_success'])\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP7Z5n_644sW",
        "outputId": "89deaa3f-cf46-4174-ea13-19254d5e8f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generalization = np.mean(cur_sum['paraphrase_success'])\n",
        "print(f'{generalization:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTn0q1Ic8CGr",
        "outputId": "1daff297-5022-441c-e7ab-65b32d26aa2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "locality = np.mean(cur_sum['neighborhood_success'])\n",
        "locality"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgNEdMGZ-SDx",
        "outputId": "4d5bbe83-9013-4bae-af06-69d4893e6ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7333333333333333\n"
          ]
        }
      ]
    }
  ]
}